{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmitShanbhoug/AmitShanbhoug/blob/main/CSI4016_Group_46_Project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CSI 4106 - Introduction to Artificial Intelligence**: Project 1: Classification Empirical Study\n",
        "\n",
        "\n",
        "---\n",
        "**Group 46**\n",
        "\n",
        "1. Feyi Adesanya, 300120992\n",
        "2. Amit Shanbhoug, 8677407\n",
        "\n",
        "Submission Date: November 1, 2022\n",
        "\n",
        "Link to Dataset: https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease\n",
        "\n",
        "Link to Google Drive Upload of Dataset: https://drive.google.com/file/d/1sKLybRum1OZlOtl3P9dXyK_DLps7y2DW/view?usp=share_link \n",
        "\n",
        "Instructions: Download the dataset \"heart_2020_cleaned.csv\" from the link and place it into the \"sample_data\" folder in google collab\n",
        "\n"
      ],
      "metadata": {
        "id": "bqMAGbzWZJ_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Understand the classification task for your dataset**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9BGHEgZtXcr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our selected dataset, *Personal Key Indicators of Heart Disease* is from a 2020 annual Center for Disease Control (CDC) survey of 400k adults in the United States (U.S), specifically from the Behavioral Risk Factor Surveillance System. \n",
        "\n",
        "This dataset contains key beheavioral indicators of heart disease in U.S adults - as an example, behavioral choices include whether an individual chooses to drink  or smoke . Through this project, we conduct a binary classification, checking whether a participant is likely to have Heart Disease in the future. We will apply three supervised machine learning algorithms to test this: Native Bayes, Multilayer Perception, and Logistic Regression\n",
        "\n",
        "\n",
        "Aside from our goal to better predict risk of Heart Disease in adults, this project will augment our ability to understand and predict behavioral risk factors in the detection of Heart Disease."
      ],
      "metadata": {
        "id": "KZpop5CzcJx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***2. Analyze your dataset***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Ti3TRqUdcwYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " *General Overview of the Dataset*\n",
        "---\n"
      ],
      "metadata": {
        "id": "vQ4JYaFZ0vZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All Necessary imports will be placed here, we will utilize scikit-learn, matplotlib, and pandas\n",
        "\n",
        "import pandas as pandas\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as seaborn\n",
        "import numpy as numpy\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Place the dataset into a format we can use: Dataframe\n",
        "\n",
        "csv_file = \"https://drive.google.com/file/d/1sKLybRum1OZlOtl3P9dXyK_DLps7y2DW/view?usp=sharing\"\n",
        "csv_file= \"'https://drive.google.com/uc?id=' + url.split('/')[-2]\"\n",
        "data_df=pandas.read_csv(csv_file)\n",
        "#data_df = pandas.read_csv(\"sample_data/heart_2020_cleaned.csv\")\n",
        "sorted(data_df)\n",
        "#print(\"Dataset Columns: \")\n",
        "print(list(data_df))\n",
        "#Show name and data type of each column in the dataset\n",
        "data_df.info()\n"
      ],
      "metadata": {
        "id": "k8t6oHcRx39V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "dd6be72e-f0a9-4b60-fe81-a96ef802f1fa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-583eb744b5da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mcsv_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://drive.google.com/file/d/1sKLybRum1OZlOtl3P9dXyK_DLps7y2DW/view?usp=sharing\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"'https://drive.google.com/uc?id=' + url.split('/')[-2]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdata_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m#data_df = pandas.read_csv(\"sample_data/heart_2020_cleaned.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             file_obj = fsspec.open(\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfsspec_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m             ).open()\n\u001b[1;32m    361\u001b[0m         \u001b[0;31m# GH 34626 Reads from Public Buckets without Credentials needs anon=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fsspec/core.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(urlpath, mode, compression, encoding, errors, protocol, newline, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m     )[0]\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fsspec/core.py\u001b[0m in \u001b[0;36mopen_files\u001b[0;34m(urlpath, mode, compression, encoding, errors, name_function, num, protocol, newline, auto_mkdir, expand, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mauto_mkdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fsspec/core.py\u001b[0m in \u001b[0;36mget_fs_token_paths\u001b[0;34m(urlpath, mode, num, name_function, storage_options, protocol, expand)\u001b[0m\n\u001b[1;32m    619\u001b[0m                         \u001b[0;34m\"share the same protocol\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                     )\n\u001b[0;32m--> 621\u001b[0;31m             \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filesystem_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m             \u001b[0moptionss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_kwargs_from_urls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murlpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strip_protocol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murlpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fsspec/registry.py\u001b[0m in \u001b[0;36mget_filesystem_class\u001b[0;34m(protocol)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mknown_implementations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Protocol not known: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m         \u001b[0mbit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknown_implementations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Protocol not known: 'https"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output showcase the 18 column in the dataset. The column labelled 'HeartDisease' is class, while the other 17 columns act as the features."
      ],
      "metadata": {
        "id": "N6iyxvK7ymb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_data = pandas.DataFrame({'total_missing': data_df.isnull().sum(), 'perc_missing': (data_df.isnull().sum()/319795)*100})\n",
        "missing_data"
      ],
      "metadata": {
        "id": "ZONwt3vxWzad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown earlier our dataset has no missing values!"
      ],
      "metadata": {
        "id": "napZkuluW0J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_df.head())"
      ],
      "metadata": {
        "id": "353Ys59CyRcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a sample of the dataset: \n",
        "\n",
        "Binary Categories - HearDisease, Smoking, AlcoholDrinking, Stroke, DiffWaking, Sex, Diabetic, PhysicalActivity, Asthema, KidneyDisease, and SkinCancer\n",
        "\n",
        "Non-Binary Categories - BMI, PhysicalHealth, MentalHealth, AgeCategory, Race, GenHealth, SleepTime,"
      ],
      "metadata": {
        "id": "Ug8BGWB6zqgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Measures For the Dataset*"
      ],
      "metadata": {
        "id": "4xMXr7oJ42lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Rows \n",
        "print(\"The total number of rows are: \",(len(data_df)))\n",
        "#Columns\n",
        "print(\"The total number of columns: \",(len(data_df.columns)))\n",
        "#Total\n",
        "print(\"the total number of data is: \",(data_df.size))\n",
        "print(\"-----------------------------------\")\n",
        "\n",
        "print(\"Numeric and statistic measures for all Continuous Features (Numeric Variables)\\n\")\n",
        "print(data_df.describe())\n"
      ],
      "metadata": {
        "id": "7oVtzEw4W_Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Previous Probabilities and Bias: What Can We Calculate Now **\n",
        "---\n"
      ],
      "metadata": {
        "id": "nTKAjobV0zQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for yesOrNo in data_df['HeartDisease'].value_counts().iteritems():\n",
        "    print(\"The Prior Probability of\", yesOrNo[0], \"is\", round((yesOrNo[1] / data_df['HeartDisease'].shape[0]*100),2),\"%\")"
      ],
      "metadata": {
        "id": "bJQAGOqw33p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***3. Brainstorm about the attributes (Feature engineering)***\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xvNyvr74cwiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset Columns: \")\n",
        "print(list(data_df))"
      ],
      "metadata": {
        "id": "j0ehIBqOyQ_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Convert Binary Columns into A Numerical Format For Graphing*\n",
        "---"
      ],
      "metadata": {
        "id": "PDNL48OB6ZSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_columns = ['HeartDisease','Smoking', 'AlcoholDrinking','Stroke','DiffWalking', 'Sex', 'PhysicalActivity', 'Asthma', 'KidneyDisease', 'SkinCancer' ]\n",
        "#Convert Binary Columns to 0 or 1\n",
        "data_df[binary_columns] = data_df[binary_columns].apply(LabelEncoder().fit_transform)\n",
        "\n",
        "#Binary Columns New Output after conversion\n",
        "for column in binary_columns:\n",
        "    print(column,data_df[column].unique())"
      ],
      "metadata": {
        "id": "qJcdNEb46Yg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Correlations*\n",
        "---\n",
        "Using the corr() function from the DataFrame library we can calculate the pairwise correlation matrix of all columns in the dataset"
      ],
      "metadata": {
        "id": "_tQqxymt5OdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pairwise Correlation of Continuous Features\n",
        "plt.figure(figsize=(12, 8))\n",
        "seaborn.heatmap(data_df.corr(), annot=True)"
      ],
      "metadata": {
        "id": "44es34dX5UPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Choosing Our Features*\n",
        "---\n",
        "We decided to keep certain features based on their relevancy to a participants general health and the affect of certain features directly on heart diease. We feel that each one picked will affect the weights our system in a manner that is relevant to the end result we want. \n",
        "\n",
        "Features Picked: HeartDisease, Smoking, AlcoholDrinking, BMI, Sex, Kidney Disease, Mental Health, Physical Health, Age Category, Race, and Physical Activity"
      ],
      "metadata": {
        "id": "C620kJN4JUJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop Unrelated Categories\n",
        "data_df.pop('Stroke')\n",
        "data_df.pop('DiffWalking')\n",
        "data_df.pop('SleepTime')\n",
        "data_df.pop('Asthma')\n",
        "data_df.pop('SkinCancer')\n",
        "data_df.pop('GenHealth')\n",
        "data_df.pop('Diabetic')\n",
        "data_df.info()"
      ],
      "metadata": {
        "id": "2tXBP2GgLSOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Exploring Our Chosen Features*\n",
        "---"
      ],
      "metadata": {
        "id": "XXWWoJT85aIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Distributions using Histograms for Numerical Features\n",
        "data_df.hist(bins=50, figsize=(20,15), color='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R0e8agMR5dCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the Distribution using a Horizontal Bar Chart for Categorical Features\n",
        "def create_barh (columnNames):\n",
        "  for col in columnNames:\n",
        "    plt.figure()\n",
        "    data_df[col].value_counts().plot(kind=\"barh\", title=col, color='r')\n",
        "    plt.show\n",
        "  \n",
        "create_barh(['Race', 'AgeCategory'])"
      ],
      "metadata": {
        "id": "WpS5_rWD5hH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***4. Encode the features***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qqf-rQo6cwuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Encoding Columns to Numerical Representations*\n",
        "---\n",
        "Binary Columns are converted to 0 and 1 using the \n",
        "\n",
        "*   Binary Columns are converted in 0 and 1s using the label encoder function (Already Done Earlier)\n",
        "*   Nominal Columns are converted using One Hot Endoding (split into multiple columns)"
      ],
      "metadata": {
        "id": "unJAt5xv5pCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert Heart Disease to True or False\n",
        "data_df['HeartDisease'] = data_df['HeartDisease'].map({1: 'TRUE', 0: 'FALSE'})\n",
        "\n",
        "#Unique Values From Each Column \n",
        "columns = data_df.columns\n",
        "for column in columns:\n",
        "    print(column,data_df[column].unique())"
      ],
      "metadata": {
        "id": "eeUGV0cI5oDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "#Encoding Columns\n",
        "onehot_categories = ['AgeCategory','Race']\n",
        "\n",
        "#Convert Binary Columns to 0 or 1, Done Above but placing code here too\n",
        "#data_df[binary_columns] = data_df[binary_columns].apply(LabelEncoder().fit_transform)\n",
        "\n",
        "#Get rid of nan values\n",
        "data_df=data_df.dropna()\n",
        "\n",
        "#Convert Categorical Data using One Hot Encoding\n",
        "new_data_df = pandas.get_dummies(data_df, columns=onehot_categories, drop_first=False)\n",
        "#data_df = pandas.get_dummies(data_df)\n",
        "\n",
        "\n",
        "print(new_data_df.head())"
      ],
      "metadata": {
        "id": "n4f2zIeJ5vOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have finished converting all Categorical data into a numerical representation with Binary Categories being assigned 0 or 1 and Nominal Categories being converted into multiple columns using One Hot Encoding"
      ],
      "metadata": {
        "id": "1oz_gAbQ5zPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***5. Prepare your data for the experiment, using cross-validation***\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9n7eaMKUcw2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Split into training and dataset*\n",
        "---"
      ],
      "metadata": {
        "id": "oE2P96uO546M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the large dataset into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "predictedClass = new_data_df.pop(\"HeartDisease\").values\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_data_df, predictedClass, test_size = 0.25, random_state=1)\n",
        "\n",
        "#Standardize Data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Size of our training and test data\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "UFkKO2-x56RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***6. Prepare your data for the experiment, using cross-validation***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2951ra-cw-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*6A: Native Bayes*\n",
        "---"
      ],
      "metadata": {
        "id": "ZyfL9CpfTUyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*6B: Logistic Regression*\n",
        "---"
      ],
      "metadata": {
        "id": "R9wkA5mJTU7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*6C: Multi Layer Perceptron*\n",
        "---"
      ],
      "metadata": {
        "id": "jN9ID0afcxE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Default Parameters Variation Test"
      ],
      "metadata": {
        "id": "yqTMtFO2C4_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing MLPClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "#Importing Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "#Initializing the Default MLPClassifier\n",
        "classifier = MLPClassifier()\n",
        "\n",
        "\n",
        "#Fitting the training data to the network, 3 runs\n",
        "print(\"Training Begins\")\n",
        "for i in range (4):\n",
        "    print(\"Training Run commenced: \", i)\n",
        "    classifier.fit(X_train, y_train)\n",
        "    print(\"the score for this run is: \",classifier.score(X_train, y_train))\n",
        "print(\"Training Done\")\n",
        "\n",
        "print()\n",
        "print(\"Final Training Score: \", classifier.score(X_train, y_train))\n",
        "print(\"Final Test Score: \", classifier.score(X_test, y_test))\n",
        "\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "print()\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "\n",
        "#Printing the accuracy\n",
        "print()\n",
        "print (f'Accuracy Score: {classifier.score(X_test,y_test):.3f}')\n",
        "\n",
        "#Plot the confusion matrix - This method has been depreciated\n",
        "#fig = plot_confusion_matrix(classifier, X_test, y_test, display_labels=classifier.classes_)\n",
        "#fig.figure_.suptitle(\"Confusion Matrix for Heart Disease Dataset MLP Default Variation \")\n",
        "#plt.show()\n",
        "print()\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm_obj = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.classes_)\n",
        "cm_obj.plot()\n",
        "plt.show()\n",
        "\n",
        "#Loss Curve\n",
        "print()\n",
        "plt.plot(classifier.loss_curve_)\n",
        "plt.title(\"Loss Curve MLP Default Variation\", fontsize=14)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eJVhJG8gLwnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Default Parameter Variation Sample of Predictions"
      ],
      "metadata": {
        "id": "9IGixuDGC_JR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predictions for Training Set\n",
        "print(\"--------Training Predictions--------\")\n",
        "y_pred_sample = classifier.predict(X_train[0:10])\n",
        "print(\"Predictions from MLP Default For First Ten Values of Training Set: \" , y_pred_sample)\n",
        "print(\"Actual Values For First Ten Values of Training Set:\", y_train[0:10])\n",
        "print(\"Array showing probability of each prediction being correct\")\n",
        "print(classifier.predict_proba(X_train[0:10]))\n",
        "\n",
        "print() #A seperator\n",
        "\n",
        "#Predictions for Test Set\n",
        "print(\"--------Test Predictions--------\")\n",
        "y_pred_sample = classifier.predict(X_test[0:10])\n",
        "print(\"Predictions from MLP Default For First Ten Values of Test Set: \" , y_pred_sample)\n",
        "print(\"Actual Values First Ten Values of Test Set:\", y_test[0:10])\n",
        "print(\"Array showing probability of each prediction being correct\")\n",
        "print(classifier.predict_proba(X_test[0:10]))\n",
        "\n"
      ],
      "metadata": {
        "id": "r0OfY8uYEDol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variation 1 Test"
      ],
      "metadata": {
        "id": "gZPMLqNjDFSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing MLPClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "#Importing Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn import metrics\n",
        "\n",
        "#Initializing the MLPClassifier with variations\n",
        "classifier_var1 = MLPClassifier(hidden_layer_sizes=(5,2), max_iter=300,activation = 'relu',solver='adam',random_state=1)\n",
        "#classifier = MLPClassifier(hidden_layer_sizes=(150,100,50), max_iter=300,activation = 'relu',solver='adam',random_state=1)\n",
        "\n",
        "#Fitting the training data to the network, 3 runs\n",
        "print(\"Training Begins\")\n",
        "for i in range (4):\n",
        "    print(\"Training Run commenced: \", i)\n",
        "    classifier_var1.fit(X_train, y_train)\n",
        "    print(\"the score for this run is: \",classifier_var1.score(X_train, y_train))\n",
        "print(\"Training Done\")\n",
        "\n",
        "print()\n",
        "print(\"Final Training Score: \", classifier_var1.score(X_train, y_train))\n",
        "print(\"Final Test Score: \", classifier_var1.score(X_test, y_test))\n",
        "\n",
        "\n",
        "y_pred = classifier_var1.predict(X_test)\n",
        "\n",
        "print()\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "\n",
        "#Printing the accuracy\n",
        "print()\n",
        "print (f'Accuracy Score: {classifier_var1.score(X_test,y_test):.3f}')\n",
        "\n",
        "# #Plot the confusion matrix\n",
        "# fig = plot_confusion_matrix(classifier_var1, X_test, y_test, display_labels=classifier_var1.classes_)\n",
        "# fig.figure_.suptitle(\"Confusion Matrix for Heart Disease Dataset MLP Variation 1\")\n",
        "# plt.show()\n",
        "print()\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm_obj = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.classes_)\n",
        "cm_obj.plot()\n",
        "plt.show()\n",
        "\n",
        "#Loss Curve for Var1\n",
        "print()\n",
        "plt.plot(classifier_var1.loss_curve_)\n",
        "plt.title(\"Loss Curve MLP Variation 1\", fontsize=14)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y8nPOEm21QPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variation 1 Sample of Predictions"
      ],
      "metadata": {
        "id": "A6fW6mAOcxLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predictions for Training Set\n",
        "print(\"--------Training Predictions--------\")\n",
        "y_pred_sample = classifier_var1.predict(X_train[0:10])\n",
        "print(\"Predictions from MLP Default For First Ten Values of Training Set: \" , y_pred_sample)\n",
        "print(\"Actual Values For First Ten Values of Training Set:\", y_train[0:10])\n",
        "print(\"Array showing probability of each prediction being correct\")\n",
        "print(classifier_var1.predict_proba(X_train[0:10]))\n",
        "\n",
        "print() #A seperator\n",
        "\n",
        "#Predictions for Test Set\n",
        "print(\"--------Test Predictions--------\")\n",
        "y_pred_sample = classifier_var1.predict(X_test[0:10])\n",
        "print(\"Predictions from MLP Default For First Ten Values of Test Set: \" , y_pred_sample)\n",
        "print(\"Actual Values For First Ten Values of Test Set:\", y_test[0:10])\n",
        "print(\"Array showing probability of each prediction being correct\")\n",
        "print(classifier_var1.predict_proba(X_test[0:10]))\n"
      ],
      "metadata": {
        "id": "DVrjdljBIxCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variation 2 Test"
      ],
      "metadata": {
        "id": "AiF0ixUFDYV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing MLPClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "#Importing Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn import metrics\n",
        "\n",
        "#Initializing the MLPClassifier with variations again\n",
        "classifier_var2 = MLPClassifier(hidden_layer_sizes=(150, 150), max_iter=100,  solver='sgd', alpha=1e-4, random_state=1, learning_rate_init=0.01, warm_start=True)\n",
        "#classifier = MLPClassifier(hidden_layer_sizes=(150,100,50), max_iter=300,activation = 'relu',solver='adam',random_state=1)\n",
        "\n",
        "\n",
        "#Fitting the training data to the network, 3 runs\n",
        "print(\"Training Begins\")\n",
        "for i in range (4):\n",
        "    print(\"Training Run commenced: \", i)\n",
        "    classifier_var2.fit(X_train, y_train)\n",
        "    print(\"the score for this run is: \",classifier_var2.score(X_train, y_train))\n",
        "print(\"Training Done\")\n",
        "\n",
        "print()\n",
        "print(\"Final Training Score: \", classifier_var2.score(X_train, y_train))\n",
        "print(\"Final Test Score: \", classifier_var2.score(X_test, y_test))\n",
        "\n",
        "\n",
        "y_pred = classifier_var2.predict(X_test)\n",
        "\n",
        "print()\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "\n",
        "#Printing the accuracy\n",
        "print()\n",
        "print (f'Accuracy Score: {classifier_var2.score(X_test,y_test):.3f}')\n",
        "\n",
        "# #Plot the confusion matrix\n",
        "# fig = plot_confusion_matrix(classifier_var2, X_test, y_test, display_labels=classifier_var2.classes_)\n",
        "# fig.figure_.suptitle(\"Confusion Matrix for Heart Disease Dataset MLP Variation 2\")\n",
        "# plt.show()\n",
        "print()\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm_obj = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.classes_)\n",
        "cm_obj.plot()\n",
        "plt.show()\n",
        "\n",
        "#Loss Curve for Var2\n",
        "print()\n",
        "plt.plot(classifier_var2.loss_curve_)\n",
        "plt.title(\"Loss Curve For MLP Variation 2\", fontsize=14)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SleCND5M1YU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variation 2 Sample of Predictions"
      ],
      "metadata": {
        "id": "hRfuGWfODc1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predictions for Training Set\n",
        "print(\"--------Training Predictions--------\")\n",
        "y_pred_sample = classifier_var2.predict(X_train[0:10])\n",
        "print(\"Predictions from MLP Default For First Ten Values of Training Set: \" , y_pred_sample)\n",
        "print(\"Actual Values For First Ten Values of Training Set:\", y_train[0:10])\n",
        "print(\"Array showing probability of each prediction being correct\")\n",
        "print(classifier_var2.predict_proba(X_train[0:10]))\n",
        "\n",
        "print() #A seperator\n",
        "\n",
        "#Predictions for Test Set\n",
        "print(\"--------Test Predictions--------\")\n",
        "y_pred_sample = classifier_var2.predict(X_test[0:10])\n",
        "print(\"Predictions from MLP Default For First Ten Values of Test Set: \" , y_pred_sample)\n",
        "print(\"Actual Values For First Ten Values of Test Set:\", y_test[0:10])\n",
        "print(\"Array showing probability of each prediction being correct\")\n",
        "print(classifier_var2.predict_proba(X_test[0:10]))"
      ],
      "metadata": {
        "id": "v7SuzVNhI4EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. Analyze the obtained results**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XLeGv3N0jcA2"
      }
    }
  ]
}